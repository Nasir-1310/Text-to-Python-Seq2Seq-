{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install missing packages (run this once)\n!pip install -q sacrebleu datasets evaluate\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport re\nimport json\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom datasets import load_dataset\nfrom sacrebleu.metrics import BLEU\nimport nltk\nnltk.download('punkt')\n\n# Device setup\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:13:57.367674Z","iopub.execute_input":"2026-02-13T05:13:57.368845Z","iopub.status.idle":"2026-02-13T05:14:01.108308Z","shell.execute_reply.started":"2026-02-13T05:13:57.368809Z","shell.execute_reply":"2026-02-13T05:14:01.107376Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\n\n# ডেটাসেট লোড\ndataset = load_dataset(\"Nan-Do/code-search-net-python\")\n\nprint(\"Available splits:\", list(dataset.keys()))          # দেখাবে: ['train']\nprint(\"Total examples:\", len(dataset['train']))           # ~455k\n\n# শুধু train split আছে → আমরা নিজেরাই split করব\nfull_train_dataset = dataset['train']\n\n# Manual split (80% train, 10% valid, 10% test)\n# shuffle + select করা ভালো\nshuffled = full_train_dataset.shuffle(seed=42)\n\ntrain_size = 8000\nvalid_size = 1000\ntest_size  = 1000\n\ntrain_data   = shuffled.select(range(0, train_size))\nvalid_data   = shuffled.select(range(train_size, train_size + valid_size))\ntest_data    = shuffled.select(range(train_size + valid_size, train_size + valid_size + test_size))\n\nprint(f\"Train: {len(train_data)} examples\")\nprint(f\"Valid: {len(valid_data)} examples\")\nprint(f\"Test:  {len(test_data)} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:20:44.348251Z","iopub.execute_input":"2026-02-13T05:20:44.348602Z","iopub.status.idle":"2026-02-13T05:20:45.118556Z","shell.execute_reply.started":"2026-02-13T05:20:44.348576Z","shell.execute_reply":"2026-02-13T05:20:45.117799Z"}},"outputs":[{"name":"stdout","text":"Available splits: ['train']\nTotal examples: 455243\nTrain: 8000 examples\nValid: 1000 examples\nTest:  1000 examples\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def clean_text(text):\n    if not text: return \"\"\n    text = text.lower().strip()\n    text = re.sub(r'\\s+', ' ', text)\n    return text\n\ndef is_valid_example(example):\n    doc = clean_text(example['docstring'])\n    code = clean_text(example['code'])\n    if not doc or not code:\n        return False\n    doc_tokens = len(doc.split())\n    code_tokens = len(code.split())\n    return 5 <= doc_tokens <= 50 and 10 <= code_tokens <= 80\n\n# Filtering\nfiltered_train = [ex for ex in tqdm(train_data) if is_valid_example(ex)]\nfiltered_valid = [ex for ex in tqdm(valid_data) if is_valid_example(ex)]\nfiltered_test  = [ex for ex in tqdm(test_data)  if is_valid_example(ex)]\n\nprint(f\"After filtering → Train: {len(filtered_train)}\")\nprint(f\"After filtering → Valid: {len(filtered_valid)}\")\nprint(f\"After filtering → Test : {len(filtered_test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:20:50.478140Z","iopub.execute_input":"2026-02-13T05:20:50.478903Z","iopub.status.idle":"2026-02-13T05:20:53.943707Z","shell.execute_reply.started":"2026-02-13T05:20:50.478873Z","shell.execute_reply":"2026-02-13T05:20:53.942981Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 8000/8000 [00:02<00:00, 2887.72it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 2913.17it/s]\n100%|██████████| 1000/1000 [00:00<00:00, 2967.23it/s]","output_type":"stream"},{"name":"stdout","text":"After filtering → Train: 4226\nAfter filtering → Valid: 532\nAfter filtering → Test : 526\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from collections import Counter\nimport re\n\n# সিম্পল টোকেনাইজার (whitespace + কিছু ক্লিনিং)\ndef simple_tokenize(text):\n    text = text.lower().strip()\n    # code এর জন্য কিছু basic cleaning (optional, পরে improve করতে পারো)\n    text = re.sub(r'([.,;:!?()])', r' \\1 ', text)   # punctuation আলাদা করা\n    text = re.sub(r'\\s+', ' ', text)\n    return text.split()\n\n# সব docstring ও code থেকে tokens কালেক্ট করা\nall_source_tokens = []\nall_target_tokens = []\n\nfor ex in tqdm(filtered_train + filtered_valid + filtered_test, desc=\"Collecting tokens\"):\n    doc_tokens = simple_tokenize(ex['docstring'])\n    code_tokens = simple_tokenize(ex['code'])\n    \n    all_source_tokens.extend(doc_tokens)\n    all_target_tokens.extend(code_tokens)\n\n# Vocabulary size নির্ধারণ (সবচেয়ে common 10k-15k words/code tokens)\nsource_counter = Counter(all_source_tokens)\ntarget_counter = Counter(all_target_tokens)\n\n# Top words নেওয়া (যেগুলো <5 বার আসে সেগুলো <unk> হবে)\nMIN_FREQ = 2   # তুমি 1 বা 3 করতে পারো\n\nsource_vocab = ['<pad>', '<sos>', '<eos>', '<unk>']\ntarget_vocab = ['<pad>', '<sos>', '<eos>', '<unk>']\n\nfor word, freq in source_counter.most_common():\n    if freq >= MIN_FREQ:\n        source_vocab.append(word)\n\nfor word, freq in target_counter.most_common():\n    if freq >= MIN_FREQ:\n        target_vocab.append(word)\n\nprint(f\"Source vocab size (docstring): {len(source_vocab)}\")\nprint(f\"Target vocab size (code):     {len(target_vocab)}\")\n\n# Mapping তৈরি\nsource_word2idx = {w: i for i, w in enumerate(source_vocab)}\nsource_idx2word = {i: w for w, i in source_word2idx.items()}\n\ntarget_word2idx = {w: i for i, w in enumerate(target_vocab)}\ntarget_idx2word = {i: w for w, i in target_word2idx.items()}\n\n# প্যাডিং index\nPAD_IDX = source_word2idx['<pad>']   # source আর target এ একই <pad> index\nSOS_IDX = source_word2idx['<sos>']\nEOS_IDX = source_word2idx['<eos>']\nUNK_IDX = source_word2idx['<unk>']\n\nprint(\"Special token indices:\", {\"<pad>\": PAD_IDX, \"<sos>\": SOS_IDX, \"<eos>\": EOS_IDX, \"<unk>\": UNK_IDX})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:25:33.074452Z","iopub.execute_input":"2026-02-13T05:25:33.074857Z","iopub.status.idle":"2026-02-13T05:25:33.569173Z","shell.execute_reply.started":"2026-02-13T05:25:33.074829Z","shell.execute_reply":"2026-02-13T05:25:33.568503Z"}},"outputs":[{"name":"stderr","text":"Collecting tokens: 100%|██████████| 5284/5284 [00:00<00:00, 14244.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Source vocab size (docstring): 4878\nTarget vocab size (code):     16362\nSpecial token indices: {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Function to convert text to list of indices using our vocabulary\ndef text_to_indices(text, word2idx, max_len, is_source=True):\n    \"\"\"\n    Convert a text string to a list of token indices.\n    - Adds <sos> at start and <eos> at end\n    - Pads or truncates to max_len\n    - Unknown words become <unk>\n    \"\"\"\n    tokens = simple_tokenize(text)  # Reuse the same tokenizer we used for vocab\n    indices = [word2idx.get(token, UNK_IDX) for token in tokens]  # <unk> for OOV\n    \n    # Truncate if too long\n    if len(indices) > max_len - 2:  # -2 for <sos> and <eos>\n        indices = indices[:max_len - 2]\n    \n    # Add <sos> and <eos>\n    indices = [SOS_IDX] + indices + [EOS_IDX]\n    \n    # Pad to max_len\n    padded = indices + [PAD_IDX] * (max_len - len(indices))\n    \n    return padded\n\n# Maximum lengths as per assignment\nMAX_DOC_LEN = 50   # docstring\nMAX_CODE_LEN = 80  # code\n\n# Custom Dataset class for PyTorch\nclass CodeGenDataset(Dataset):\n    \"\"\"\n    PyTorch Dataset class for our docstring → code pairs.\n    Returns source (docstring) and target (code) as tensors.\n    \"\"\"\n    def __init__(self, data_list):\n        self.data = data_list\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        example = self.data[idx]\n        \n        # Convert docstring (source)\n        src = text_to_indices(example['docstring'], source_word2idx, MAX_DOC_LEN, is_source=True)\n        \n        # Convert code (target)\n        tgt = text_to_indices(example['code'], target_word2idx, MAX_CODE_LEN, is_source=False)\n        \n        return {\n            'src': torch.tensor(src, dtype=torch.long),\n            'tgt': torch.tensor(tgt, dtype=torch.long)\n        }\n\n# Create datasets\ntrain_dataset = CodeGenDataset(filtered_train)\nvalid_dataset = CodeGenDataset(filtered_valid)\ntest_dataset  = CodeGenDataset(filtered_test)\n\nprint(f\"Train dataset size: {len(train_dataset)}\")\nprint(f\"Valid dataset size: {len(valid_dataset)}\")\nprint(f\"Test  dataset size: {len(test_dataset)}\")\n\n# Example: Check first sample\nsample = train_dataset[0]\nprint(\"\\nSample source (docstring indices):\", sample['src'].tolist())\nprint(\"Sample target (code indices):    \", sample['tgt'].tolist())\nprint(\"Source length:\", len(sample['src']))\nprint(\"Target length:\", len(sample['tgt']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:30:08.421964Z","iopub.execute_input":"2026-02-13T05:30:08.422648Z","iopub.status.idle":"2026-02-13T05:30:08.448817Z","shell.execute_reply.started":"2026-02-13T05:30:08.422617Z","shell.execute_reply":"2026-02-13T05:30:08.448023Z"}},"outputs":[{"name":"stdout","text":"Train dataset size: 4226\nValid dataset size: 532\nTest  dataset size: 526\n\nSample source (docstring indices): [1, 681, 7, 444, 1961, 5, 4, 13, 34, 3, 4, 6, 431, 9, 6, 444, 1961, 5, 4, 13, 57, 205, 4, 6, 71, 205, 14, 6, 71, 5, 4, 15, 4, 6, 891, 444, 1961, 5, 4, 58, 4, 57, 2, 0, 0, 0, 0, 0, 0, 0]\nSample target (code indices):     [1, 15, 3, 6, 9, 8, 4591, 8, 3749, 5, 7, 3169, 17, 855, 4592, 4, 7, 28, 43, 4591, 7, 11, 500, 19, 11, 855, 4592, 4, 7, 28, 79, 256, 7, 11, 76, 256, 18, 11, 76, 4, 7, 13, 7, 11, 1013, 855, 4592, 4, 7, 132, 7, 79, 12, 14, 256, 21, 24, 7, 256, 10, 93, 31, 10, 313, 4, 6150, 6, 4591, 5, 9068, 10, 313, 4, 6151, 6, 4591, 5, 13, 9, 2]\nSource length: 50\nTarget length: 80\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef collate_fn(batch):\n    \"\"\"\n    Custom collate function to pad sequences in a batch dynamically.\n    Returns padded src, tgt, and their lengths.\n    \"\"\"\n    src_batch = [item['src'] for item in batch]\n    tgt_batch = [item['tgt'] for item in batch]\n    \n    # Pad sequences (already padded to max, but we keep it simple)\n    # Since we already padded to fixed MAX in __getitem__, no extra pad needed\n    # But we can compute lengths if needed later\n    src_lengths = torch.tensor([torch.sum(item['src'] != PAD_IDX).item() for item in batch])\n    tgt_lengths = torch.tensor([torch.sum(item['tgt'] != PAD_IDX).item() for item in batch])\n    \n    src = torch.stack(src_batch)   # [batch_size, MAX_DOC_LEN]\n    tgt = torch.stack(tgt_batch)   # [batch_size, MAX_CODE_LEN]\n    \n    return {\n        'src': src,\n        'tgt': tgt,\n        'src_len': src_lengths,\n        'tgt_len': tgt_lengths\n    }\n\n# Create DataLoaders\nBATCH_SIZE = 32   # GPU memory অনুযায়ী 16/32/64 করতে পারো\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    collate_fn=collate_fn,\n    num_workers=2   # Kaggle-এ 2-4 ভালো\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=2\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=2\n)\n\nprint(f\"Number of train batches: {len(train_loader)}\")\nprint(f\"Number of valid batches: {len(valid_loader)}\")\nprint(f\"Number of test batches : {len(test_loader)}\")\n\n# Check one batch shape\nbatch = next(iter(train_loader))\nprint(\"\\nBatch shapes:\")\nprint(\"src:\", batch['src'].shape)      # [batch_size, 50]\nprint(\"tgt:\", batch['tgt'].shape)      # [batch_size, 80]\nprint(\"src_len example:\", batch['src_len'][:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:32:34.912445Z","iopub.execute_input":"2026-02-13T05:32:34.913299Z","iopub.status.idle":"2026-02-13T05:32:35.082351Z","shell.execute_reply.started":"2026-02-13T05:32:34.913258Z","shell.execute_reply":"2026-02-13T05:32:35.081364Z"}},"outputs":[{"name":"stdout","text":"Number of train batches: 133\nNumber of valid batches: 17\nNumber of test batches : 17\n\nBatch shapes:\nsrc: torch.Size([32, 50])\ntgt: torch.Size([32, 80])\nsrc_len example: tensor([32,  8, 50, 30, 14])\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import torch.nn as nn\n\nclass EncoderRNN(nn.Module):\n    \"\"\"\n    Vanilla RNN Encoder\n    - Takes source sequence (docstring)\n    - Returns final hidden state as context vector\n    \"\"\"\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1, dropout=0.0):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.RNN(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=n_layers,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True\n        )\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, src):\n        # src = [batch_size, src_len]\n        embedded = self.dropout(self.embedding(src))          # [batch_size, src_len, emb_dim]\n        outputs, hidden = self.rnn(embedded)                   # hidden = [n_layers, batch_size, hid_dim]\n        return hidden  # We only need the last hidden state\n\n\nclass DecoderRNN(nn.Module):\n    \"\"\"\n    Vanilla RNN Decoder\n    - Takes context vector + previous token\n    - Predicts next token\n    \"\"\"\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=1, dropout=0.0):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.RNN(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=n_layers,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True\n        )\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input, hidden):\n        # input = [batch_size]  → single token index\n        # hidden = [n_layers, batch_size, hid_dim]\n        input = input.unsqueeze(1)                                 # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input))            # [batch_size, 1, emb_dim]\n        output, hidden = self.rnn(embedded, hidden)                # output: [batch_size, 1, hid_dim]\n        prediction = self.fc_out(output.squeeze(1))               # [batch_size, output_dim]\n        return prediction, hidden\n\n\nclass Seq2SeqRNN(nn.Module):\n    \"\"\"\n    Full Vanilla RNN Seq2Seq Model\n    \"\"\"\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        # src = [batch_size, src_len]\n        # trg = [batch_size, trg_len]\n        batch_size = src.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = len(target_vocab)\n        \n        # Tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        \n        # Encoder forward\n        hidden = self.encoder(src)   # [1, batch_size, hid_dim]  (n_layers=1)\n        \n        # First input to decoder is <sos>\n        input = trg[:, 0]   # [batch_size]\n        \n        for t in range(1, trg_len):\n            output, hidden = self.decoder(input, hidden)\n            outputs[:, t, :] = output\n            \n            # Decide if we use teacher forcing\n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)   # [batch_size]\n            \n            input = trg[:, t] if teacher_force else top1\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:35:24.628144Z","iopub.execute_input":"2026-02-13T05:35:24.628496Z","iopub.status.idle":"2026-02-13T05:35:24.639454Z","shell.execute_reply.started":"2026-02-13T05:35:24.628464Z","shell.execute_reply":"2026-02-13T05:35:24.638802Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Common hyperparameters for all models\nEMB_DIM = 256\nHID_DIM = 256\nN_LAYERS = 1\nDROPOUT = 0.1\nDEVICE = device\n\n# Initialize Vanilla RNN model\nINPUT_DIM = len(source_vocab)\nOUTPUT_DIM = len(target_vocab)\n\nenc_rnn = EncoderRNN(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\ndec_rnn = DecoderRNN(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n\nmodel_rnn = Seq2SeqRNN(enc_rnn, dec_rnn, DEVICE).to(DEVICE)\n\nprint(model_rnn)\nprint(f\"Total parameters: {sum(p.numel() for p in model_rnn.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:36:36.778447Z","iopub.execute_input":"2026-02-13T05:36:36.778797Z","iopub.status.idle":"2026-02-13T05:36:37.237111Z","shell.execute_reply.started":"2026-02-13T05:36:36.778771Z","shell.execute_reply":"2026-02-13T05:36:37.236184Z"}},"outputs":[{"name":"stdout","text":"Seq2SeqRNN(\n  (encoder): EncoderRNN(\n    (embedding): Embedding(4878, 256, padding_idx=0)\n    (rnn): RNN(256, 256, batch_first=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): DecoderRNN(\n    (embedding): Embedding(16362, 256, padding_idx=0)\n    (rnn): RNN(256, 256, batch_first=True)\n    (fc_out): Linear(in_features=256, out_features=16362, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)\nTotal parameters: 9,905,642\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch.optim as optim\nfrom tqdm import tqdm\nimport time\nimport math\n\n# Loss function: Cross-entropy, ignore padding tokens\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\n# Optimizer: Adam (assignment-এ বলা আছে)\noptimizer = optim.Adam(model_rnn.parameters(), lr=0.001)\n\n# Function to calculate time taken\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n# Training function (one epoch)\ndef train(model, iterator, optimizer, criterion, clip=1):\n    model.train()\n    \n    epoch_loss = 0\n    \n    for batch in tqdm(iterator, desc=\"Training\"):\n        src = batch['src'].to(DEVICE)\n        trg = batch['tgt'].to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)           # [batch_size, trg_len, output_dim]\n        \n        # Reshape for loss calculation\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)   # remove <sos>\n        trg = trg[:, 1:].reshape(-1)                      # remove <sos>\n        \n        loss = criterion(output, trg)\n        \n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n    \n    return epoch_loss / len(iterator)\n\n\n# Evaluation function (validation / test)\ndef evaluate(model, iterator, criterion):\n    model.eval()\n    \n    epoch_loss = 0\n    \n    with torch.no_grad():\n        for batch in tqdm(iterator, desc=\"Evaluating\"):\n            src = batch['src'].to(DEVICE)\n            trg = batch['tgt'].to(DEVICE)\n            \n            output = model(src, trg, teacher_forcing_ratio=0.0)  # no teacher forcing\n            \n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            trg = trg[:, 1:].reshape(-1)\n            \n            loss = criterion(output, trg)\n            \n            epoch_loss += loss.item()\n    \n    return epoch_loss / len(iterator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:40:22.553774Z","iopub.execute_input":"2026-02-13T05:40:22.554171Z","iopub.status.idle":"2026-02-13T05:40:25.347444Z","shell.execute_reply.started":"2026-02-13T05:40:22.554143Z","shell.execute_reply":"2026-02-13T05:40:25.346777Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"N_EPOCHS = 8\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nprint(\"Starting Vanilla RNN training...\\n\")\n\nfor epoch in range(N_EPOCHS):\n    start_time = time.time()\n    \n    train_loss = train(model_rnn, train_loader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model_rnn, valid_loader, criterion)\n    \n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model_rnn.state_dict(), 'vanilla_rnn_best.pt')\n        print(\"** Saved best model **\")\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:49:43.755021Z","iopub.execute_input":"2026-02-13T05:49:43.755437Z","iopub.status.idle":"2026-02-13T05:56:16.076218Z","shell.execute_reply.started":"2026-02-13T05:49:43.755381Z","shell.execute_reply":"2026-02-13T05:56:16.075284Z"}},"outputs":[{"name":"stdout","text":"Starting Vanilla RNN training...\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.88it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"** Saved best model **\nEpoch: 01 | Time: 0m 48s\n\tTrain Loss: 4.612 | Train PPL: 100.667\n\t Val. Loss: 5.835 |  Val. PPL: 342.025\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.87it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 02 | Time: 0m 49s\n\tTrain Loss: 4.554 | Train PPL:  95.052\n\t Val. Loss: 5.853 |  Val. PPL: 348.153\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.85it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 03 | Time: 0m 49s\n\tTrain Loss: 4.526 | Train PPL:  92.404\n\t Val. Loss: 5.877 |  Val. PPL: 356.717\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.88it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"** Saved best model **\nEpoch: 04 | Time: 0m 48s\n\tTrain Loss: 4.490 | Train PPL:  89.143\n\t Val. Loss: 5.776 |  Val. PPL: 322.601\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.88it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 05 | Time: 0m 48s\n\tTrain Loss: 4.443 | Train PPL:  85.036\n\t Val. Loss: 5.854 |  Val. PPL: 348.713\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.84it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 06 | Time: 0m 49s\n\tTrain Loss: 4.415 | Train PPL:  82.702\n\t Val. Loss: 5.824 |  Val. PPL: 338.309\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.87it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 07 | Time: 0m 48s\n\tTrain Loss: 4.395 | Train PPL:  81.006\n\t Val. Loss: 5.929 |  Val. PPL: 375.932\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.87it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.54it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 08 | Time: 0m 48s\n\tTrain Loss: 4.362 | Train PPL:  78.416\n\t Val. Loss: 5.989 |  Val. PPL: 399.024\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class EncoderLSTM(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.lstm = nn.LSTM(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=n_layers,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True\n        )\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.lstm(embedded)\n        # hidden & cell: [n_layers, batch_size, hid_dim]\n        return hidden, cell\n\n\nclass DecoderLSTM(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=1, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n        self.lstm = nn.LSTM(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=n_layers,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True\n        )\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input, hidden, cell):\n        input = input.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.dropout(self.embedding(input))\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc_out(output.squeeze(1))\n        return prediction, hidden, cell\n\n\nclass Seq2SeqLSTM(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = len(target_vocab)\n        \n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        \n        hidden, cell = self.encoder(src)  # both [1, batch_size, hid_dim]\n        \n        input = trg[:, 0]  # <sos>\n        \n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[:, t, :] = output\n            \n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            \n            input = trg[:, t] if teacher_force else top1\n        \n        return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:00:08.869548Z","iopub.execute_input":"2026-02-13T06:00:08.870640Z","iopub.status.idle":"2026-02-13T06:00:08.882810Z","shell.execute_reply.started":"2026-02-13T06:00:08.870604Z","shell.execute_reply":"2026-02-13T06:00:08.881973Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# LSTM model\nenc_lstm = EncoderLSTM(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\ndec_lstm = DecoderLSTM(OUTPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n\nmodel_lstm = Seq2SeqLSTM(enc_lstm, dec_lstm, DEVICE).to(DEVICE)\n\nprint(model_lstm)\nprint(f\"LSTM Total parameters: {sum(p.numel() for p in model_lstm.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:01:08.729758Z","iopub.execute_input":"2026-02-13T06:01:08.730116Z","iopub.status.idle":"2026-02-13T06:01:08.862144Z","shell.execute_reply.started":"2026-02-13T06:01:08.730085Z","shell.execute_reply":"2026-02-13T06:01:08.861448Z"}},"outputs":[{"name":"stdout","text":"Seq2SeqLSTM(\n  (encoder): EncoderLSTM(\n    (embedding): Embedding(4878, 256, padding_idx=0)\n    (lstm): LSTM(256, 256, batch_first=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): DecoderLSTM(\n    (embedding): Embedding(16362, 256, padding_idx=0)\n    (lstm): LSTM(256, 256, batch_first=True)\n    (fc_out): Linear(in_features=256, out_features=16362, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)\nLSTM Total parameters: 10,695,146\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# LSTM model এর জন্য নতুন optimizer ও loss (পুরানোটা RNN-এর জন্য ছিল)\ncriterion_lstm = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\noptimizer_lstm = optim.Adam(model_lstm.parameters(), lr=0.001)\n\nprint(\"Starting LSTM Seq2Seq training...\\n\")\n\nN_EPOCHS_LSTM = 10          # LSTM-এর জন্য একটু বেশি epoch দিচ্ছি\nbest_valid_loss_lstm = float('inf')\n\nfor epoch in range(N_EPOCHS_LSTM):\n    start_time = time.time()\n    \n    train_loss = train(model_lstm, train_loader, optimizer_lstm, criterion_lstm, CLIP)\n    valid_loss = evaluate(model_lstm, valid_loader, criterion_lstm)\n    \n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss_lstm:\n        best_valid_loss_lstm = valid_loss\n        torch.save(model_lstm.state_dict(), 'lstm_best.pt')\n        print(\"** Saved best LSTM model **\")\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T06:05:21.965102Z","iopub.execute_input":"2026-02-13T06:05:21.965467Z","iopub.status.idle":"2026-02-13T06:13:38.975783Z","shell.execute_reply.started":"2026-02-13T06:05:21.965437Z","shell.execute_reply":"2026-02-13T06:13:38.974904Z"}},"outputs":[{"name":"stdout","text":"Starting LSTM Seq2Seq training...\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.83it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"** Saved best LSTM model **\nEpoch: 01 | Time: 0m 49s\n\tTrain Loss: 5.857 | Train PPL: 349.768\n\t Val. Loss: 5.599 |  Val. PPL: 270.125\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.84it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"** Saved best LSTM model **\nEpoch: 02 | Time: 0m 49s\n\tTrain Loss: 5.059 | Train PPL: 157.507\n\t Val. Loss: 5.563 |  Val. PPL: 260.538\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:47<00:00,  2.83it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 03 | Time: 0m 49s\n\tTrain Loss: 4.873 | Train PPL: 130.670\n\t Val. Loss: 5.584 |  Val. PPL: 266.126\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.83it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 04 | Time: 0m 49s\n\tTrain Loss: 4.761 | Train PPL: 116.824\n\t Val. Loss: 5.604 |  Val. PPL: 271.439\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.86it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 05 | Time: 0m 49s\n\tTrain Loss: 4.679 | Train PPL: 107.651\n\t Val. Loss: 5.576 |  Val. PPL: 264.054\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:46<00:00,  2.84it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 06 | Time: 0m 49s\n\tTrain Loss: 4.603 | Train PPL:  99.800\n\t Val. Loss: 5.656 |  Val. PPL: 285.959\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:47<00:00,  2.82it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 07 | Time: 0m 49s\n\tTrain Loss: 4.542 | Train PPL:  93.838\n\t Val. Loss: 5.659 |  Val. PPL: 286.912\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:47<00:00,  2.82it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 08 | Time: 0m 49s\n\tTrain Loss: 4.507 | Train PPL:  90.628\n\t Val. Loss: 5.813 |  Val. PPL: 334.513\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:47<00:00,  2.81it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 09 | Time: 0m 49s\n\tTrain Loss: 4.460 | Train PPL:  86.519\n\t Val. Loss: 5.712 |  Val. PPL: 302.536\n\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 133/133 [00:47<00:00,  2.82it/s]\nEvaluating: 100%|██████████| 17/17 [00:02<00:00,  6.38it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 10 | Time: 0m 49s\n\tTrain Loss: 4.433 | Train PPL:  84.185\n\t Val. Loss: 5.581 |  Val. PPL: 265.268\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class BahdanauAttention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        self.Wa = nn.Linear(dec_hid_dim, enc_hid_dim)           # query → enc_hid_dim space\n        self.Ua = nn.Linear(enc_hid_dim, enc_hid_dim)           # keys → enc_hid_dim space\n        self.Va = nn.Linear(enc_hid_dim, 1)\n    \n    def forward(self, query, keys):\n        # query:   [batch_size, dec_hid_dim]\n        # keys:    [batch_size, src_len, enc_hid_dim]\n        query_proj = self.Wa(query).unsqueeze(1)               # [batch, 1, enc_hid_dim]\n        keys_proj  = self.Ua(keys)                             # [batch, src_len, enc_hid_dim]\n        \n        energy = torch.tanh(query_proj + keys_proj)            # [batch, src_len, enc_hid_dim]\n        attention_scores = self.Va(energy).squeeze(2)          # [batch, src_len]\n        \n        return F.softmax(attention_scores, dim=1)              # [batch, src_len]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T07:08:21.637524Z","iopub.execute_input":"2026-02-13T07:08:21.638181Z","iopub.status.idle":"2026-02-13T07:08:21.644138Z","shell.execute_reply.started":"2026-02-13T07:08:21.638151Z","shell.execute_reply":"2026-02-13T07:08:21.643191Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"class EncoderBiLSTM(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.lstm = nn.LSTM(\n            emb_dim,\n            hid_dim,\n            num_layers=n_layers,\n            bidirectional=True,\n            dropout=dropout if n_layers > 1 else 0,\n            batch_first=True\n        )\n        self.dropout = nn.Dropout(dropout)\n        \n        # Projection layers: bidirectional hidden → decoder hidden size\n        self.hidden_projection = nn.Linear(hid_dim * 2, hid_dim)\n        self.cell_projection   = nn.Linear(hid_dim * 2, hid_dim)\n    \n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.lstm(embedded)\n\n        # hidden, cell: [2, batch, hid_dim]  (bidirectional)\n        # Concat forward + backward\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)   # [batch, hid*2]\n        cell   = torch.cat((cell[-2,:,:],   cell[-1,:,:]),   dim=1)\n\n        # Project to decoder hidden size\n        hidden = self.hidden_projection(hidden)   # [batch, hid_dim]\n        cell   = self.cell_projection(cell)       # [batch, hid_dim]\n\n        # MOST IMPORTANT: add num_layers dimension (1)\n        hidden = hidden.unsqueeze(0)              # [1, batch, hid_dim]\n        cell   = cell.unsqueeze(0)                # [1, batch, hid_dim]\n\n        return outputs, hidden, cell","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T07:09:48.338523Z","iopub.execute_input":"2026-02-13T07:09:48.339296Z","iopub.status.idle":"2026-02-13T07:09:48.347080Z","shell.execute_reply.started":"2026-02-13T07:09:48.339267Z","shell.execute_reply":"2026-02-13T07:09:48.346192Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"class DecoderAttentionLSTM(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, enc_hid_dim, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n        self.attention = BahdanauAttention(enc_hid_dim, hid_dim)\n        \n        # LSTM input dim = embedding + context (enc_hid_dim)\n        self.lstm = nn.LSTM(\n            emb_dim + enc_hid_dim,\n            hid_dim,\n            num_layers=1,               # fixed to 1\n            batch_first=True,\n            dropout=0                   # n_layers=1 বলে dropout 0\n        )\n        \n        self.fc_out = nn.Linear(emb_dim + hid_dim + enc_hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, input, hidden, cell, encoder_outputs):\n        # input: [batch_size]\n        input = input.unsqueeze(1)                             # [batch, 1]\n        embedded = self.dropout(self.embedding(input))         # [batch, 1, emb]\n        \n        # Attention\n        attn_weights = self.attention(hidden, encoder_outputs)  # [batch, src_len]\n        attn_weights = attn_weights.unsqueeze(1)                # [batch, 1, src_len]\n        \n        context = torch.bmm(attn_weights, encoder_outputs)      # [batch, 1, enc_hid_dim]\n        \n        # LSTM input\n        lstm_input = torch.cat((embedded, context), dim=2)     # [batch, 1, emb + enc_hid]\n        \n        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n        \n        # Prediction\n        embedded = embedded.squeeze(1)\n        output   = output.squeeze(1)\n        context  = context.squeeze(1)\n        \n        pred_input = torch.cat((embedded, output, context), dim=1)\n        prediction = self.fc_out(pred_input)\n        \n        return prediction, hidden, cell, attn_weights.squeeze(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T07:09:52.991650Z","iopub.execute_input":"2026-02-13T07:09:52.991944Z","iopub.status.idle":"2026-02-13T07:09:52.999891Z","shell.execute_reply.started":"2026-02-13T07:09:52.991919Z","shell.execute_reply":"2026-02-13T07:09:52.999322Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"class Seq2SeqAttention(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n    \n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = src.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = len(target_vocab)\n        \n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        attentions = torch.zeros(batch_size, trg_len, src.shape[1]).to(self.device)\n        \n        enc_outputs, hidden, cell = self.encoder(src)\n        \n        input = trg[:, 0]  # <sos>\n        \n        for t in range(1, trg_len):\n            output, hidden, cell, attn = self.decoder(input, hidden, cell, enc_outputs)\n            outputs[:, t, :] = output\n            attentions[:, t, :] = attn\n            \n            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            \n            input = trg[:, t] if teacher_force else top1\n        \n        return outputs, attentions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T07:10:02.958342Z","iopub.execute_input":"2026-02-13T07:10:02.958958Z","iopub.status.idle":"2026-02-13T07:10:02.965944Z","shell.execute_reply.started":"2026-02-13T07:10:02.958927Z","shell.execute_reply":"2026-02-13T07:10:02.965072Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"enc_attn = EncoderBiLSTM(INPUT_DIM, EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\ndec_attn = DecoderAttentionLSTM(OUTPUT_DIM, EMB_DIM, HID_DIM, HID_DIM*2, DROPOUT)\n\nmodel_attn = Seq2SeqAttention(enc_attn, dec_attn, DEVICE).to(DEVICE)\n\noptimizer_attn = optim.Adam(model_attn.parameters(), lr=0.0005)\nprint(f\"Attention Model parameters: {sum(p.numel() for p in model_attn.parameters() if p.requires_grad):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T07:13:31.708359Z","iopub.execute_input":"2026-02-13T07:13:31.709193Z","iopub.status.idle":"2026-02-13T07:13:31.945630Z","shell.execute_reply.started":"2026-02-13T07:13:31.709159Z","shell.execute_reply":"2026-02-13T07:13:31.944908Z"}},"outputs":[{"name":"stdout","text":"Attention Model parameters: 24,969,195\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# যদি memory error না আসে তাহলে BATCH_SIZE = 16 বা 24 রাখো\n# যদি আসে তাহলে 12 বা 8 করে দাও\n\nN_EPOCHS_ATTN = 10   # প্রথমে ১০ দিয়ে দেখো, পরে বাড়াতে পারো\n\nbest_valid_loss_attn = float('inf')\n\nprint(\"Starting Attention Model training...\\n\")\n\nfor epoch in range(N_EPOCHS_ATTN):\n    start_time = time.time()\n    \n    train_loss = train_attention(model_attn, train_loader, optimizer_attn, criterion_attn, clip=1)\n    valid_loss, sample_attns = evaluate_attention(model_attn, valid_loader, criterion_attn)\n    \n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss_attn:\n        best_valid_loss_attn = valid_loss\n        torch.save(model_attn.state_dict(), 'attention_best.pt')\n        print(\"** Saved best Attention model **\")\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T07:13:36.565386Z","iopub.execute_input":"2026-02-13T07:13:36.566007Z","iopub.status.idle":"2026-02-13T07:13:36.804017Z","shell.execute_reply.started":"2026-02-13T07:13:36.565978Z","shell.execute_reply":"2026-02-13T07:13:36.802822Z"}},"outputs":[{"name":"stdout","text":"Starting Attention Model training...\n\n","output_type":"stream"},{"name":"stderr","text":"Training Attention:   0%|          | 0/133 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/323830740.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_attns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3449121181.py\u001b[0m in \u001b[0;36mtrain_attention\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# attentions not needed during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/3308191327.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mattentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/1191676393.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, cell, encoder_outputs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, src_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m                \u001b[0;31m# [batch, 1, src_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/318944493.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, keys)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mkeys_proj\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m                             \u001b[0;31m# [batch, src_len, enc_hid_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0menergy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_proj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeys_proj\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# [batch, src_len, enc_hid_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# [batch, src_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (50) at non-singleton dimension 2"],"ename":"RuntimeError","evalue":"The size of tensor a (32) must match the size of tensor b (50) at non-singleton dimension 2","output_type":"error"}],"execution_count":79},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}